{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text from https://github.com/assafelovic/gpt-researcher/blob/master/gpt_researcher/scraper/scraper.py\n",
    "from concurrent.futures.thread import ThreadPoolExecutor\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.retrievers import ArxivRetriever\n",
    "from functools import partial\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "class Scraper:\n",
    "    \"\"\"\n",
    "    Scraper class to extract the content from the links\n",
    "    \"\"\"\n",
    "    def __init__(self, urls, user_agent):\n",
    "        \"\"\"\n",
    "        Initialize the Scraper class.\n",
    "        Args:\n",
    "            urls:\n",
    "        \"\"\"\n",
    "        self.urls = urls\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            \"User-Agent\": user_agent\n",
    "        })\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Extracts the content from the links\n",
    "        \"\"\"\n",
    "        partial_extract = partial(self.extract_data_from_link, session=self.session)\n",
    "        with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "            contents = executor.map(partial_extract, self.urls)\n",
    "        res = [content for content in contents if content['raw_content'] is not None]\n",
    "        return res\n",
    "\n",
    "    def extract_data_from_link(self, link, session):\n",
    "        \"\"\"\n",
    "        Extracts the data from the link\n",
    "        \"\"\"\n",
    "        content = \"\"\n",
    "        try:\n",
    "            if link.endswith(\".pdf\"):\n",
    "                content = self.scrape_pdf_with_pymupdf(link)\n",
    "            elif \"arxiv.org\" in link:\n",
    "                doc_num = link.split(\"/\")[-1]\n",
    "                content = self.scrape_pdf_with_arxiv(doc_num)\n",
    "            elif link:\n",
    "                content = self.scrape_text_with_bs(link, session)\n",
    "\n",
    "            if len(content) < 100:\n",
    "                return {'url': link, 'raw_content': None}\n",
    "            return {'url': link, 'raw_content': content}\n",
    "        except Exception as e:\n",
    "            return {'url': link, 'raw_content': None}\n",
    "\n",
    "    def scrape_text_with_bs(self, link, session):\n",
    "        response = session.get(link, timeout=4)\n",
    "        soup = BeautifulSoup(response.content, 'lxml', from_encoding=response.encoding)\n",
    "\n",
    "        for script_or_style in soup([\"script\", \"style\"]):\n",
    "            script_or_style.extract()\n",
    "\n",
    "        raw_content = self.get_content_from_url(soup)\n",
    "        lines = (line.strip() for line in raw_content.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        content = \"\\n\".join(chunk for chunk in chunks if chunk)\n",
    "        return content\n",
    "\n",
    "    def scrape_pdf_with_pymupdf(self, url) -> str:\n",
    "        \"\"\"Scrape a pdf with pymupdf\n",
    "\n",
    "        Args:\n",
    "            url (str): The url of the pdf to scrape\n",
    "\n",
    "        Returns:\n",
    "            str: The text scraped from the pdf\n",
    "        \"\"\"\n",
    "        loader = PyMuPDFLoader(url)\n",
    "        doc = loader.load()\n",
    "        return str(doc)\n",
    "\n",
    "    def scrape_pdf_with_arxiv(self, query) -> str:\n",
    "        \"\"\"Scrape a pdf with arxiv\n",
    "        default document length of 70000 about ~15 pages or None for no limit\n",
    "\n",
    "        Args:\n",
    "            query (str): The query to search for\n",
    "\n",
    "        Returns:\n",
    "            str: The text scraped from the pdf\n",
    "        \"\"\"\n",
    "        retriever = ArxivRetriever(load_max_docs=2, doc_content_chars_max=None)\n",
    "        docs = retriever.get_relevant_documents(query=query)\n",
    "        return docs[0].page_content\n",
    "\n",
    "    def get_content_from_url(self, soup):\n",
    "        \"\"\"Get the text from the soup\n",
    "\n",
    "        Args:\n",
    "            soup (BeautifulSoup): The soup to get the text from\n",
    "\n",
    "        Returns:\n",
    "            str: The text from the soup\n",
    "        \"\"\"\n",
    "        text = \"\"\n",
    "        tags = ['p', 'h1', 'h2', 'h3', 'h4', 'h5']\n",
    "        for element in soup.find_all(tags):  # Find all the <p> elements\n",
    "            text += element.text + \"\\n\"\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text from https://github.com/assafelovic/gpt-researcher/blob/master/gpt_researcher/retrievers/duckduckgo/duckduckgo.py\n",
    "from itertools import islice\n",
    "from duckduckgo_search import DDGS\n",
    "\n",
    "\n",
    "class Duckduckgo:\n",
    "    \"\"\"\n",
    "    Duckduckgo API Retriever\n",
    "    \"\"\"\n",
    "    def __init__(self, query):\n",
    "        self.ddg = DDGS()\n",
    "        self.query = query\n",
    "\n",
    "    def search(self, max_results=5):\n",
    "        \"\"\"\n",
    "        Performs the search\n",
    "        :param query:\n",
    "        :param max_results:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        ddgs_gen = self.ddg.text(self.query, region='wt-wt', max_results=max_results)\n",
    "        return ddgs_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tavily API Retriever\n",
    "\n",
    "# libraries\n",
    "import os\n",
    "from tavily import TavilyClient\n",
    "from duckduckgo_search import DDGS\n",
    "\n",
    "\n",
    "class TavilySearch():\n",
    "    \"\"\"\n",
    "    Tavily API Retriever\n",
    "    \"\"\"\n",
    "    def __init__(self, query):\n",
    "        \"\"\"\n",
    "        Initializes the TavilySearch object\n",
    "        Args:\n",
    "            query:\n",
    "        \"\"\"\n",
    "        self.query = query\n",
    "        self.api_key = self.get_api_key()\n",
    "        self.client = TavilyClient(self.api_key)\n",
    "\n",
    "    def get_api_key(self):\n",
    "        \"\"\"\n",
    "        Gets the Tavily API key\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        # Get the API key\n",
    "        try:\n",
    "            api_key = os.environ[\"TAVILY_API_KEY\"]\n",
    "        except:\n",
    "            raise Exception(\"Tavily API key not found. Please set the TAVILY_API_KEY environment variable. \"\n",
    "                            \"You can get a key at https://app.tavily.com\")\n",
    "        return api_key\n",
    "\n",
    "    def search(self, max_results=7):\n",
    "        \"\"\"\n",
    "        Searches the query\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Search the query\n",
    "            results = self.client.search(self.query, search_depth=\"advanced\", max_results=max_results)\n",
    "            # Return the results\n",
    "            search_response = [{\"href\": obj[\"url\"], \"body\": obj[\"content\"]} for obj in results.get(\"results\", [])]\n",
    "        except Exception as e: # Fallback in case overload on Tavily Search API\n",
    "            ddg = DDGS()\n",
    "            search_response = ddg.text(self.query, region='wt-wt', max_results=max_results)\n",
    "        return search_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tavily API Retriever\n",
    "\n",
    "# libraries\n",
    "import os\n",
    "from tavily import TavilyClient\n",
    "\n",
    "\n",
    "class TavilyNews():\n",
    "    \"\"\"\n",
    "    Tavily News API Retriever\n",
    "    Retrieve news articles from the Tavily News API\n",
    "    \"\"\"\n",
    "    def __init__(self, query):\n",
    "        \"\"\"\n",
    "        Initializes the TavilySearch object\n",
    "        Args:\n",
    "            query:\n",
    "        \"\"\"\n",
    "        self.query = query\n",
    "        self.api_key = self.get_api_key()\n",
    "        self.client = TavilyClient(self.api_key)\n",
    "\n",
    "    def get_api_key(self):\n",
    "        \"\"\"\n",
    "        Gets the Tavily API key\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        # Get the API key\n",
    "        try:\n",
    "            api_key = os.environ[\"TAVILY_API_KEY\"]\n",
    "        except:\n",
    "            raise Exception(\"Tavily API key not found. Please set the TAVILY_API_KEY environment variable. \"\n",
    "                            \"You can get a key at https://app.tavily.com\")\n",
    "        return api_key\n",
    "\n",
    "    def search(self, max_results=7):\n",
    "        \"\"\"\n",
    "        Searches the query\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        # Search the query\n",
    "        results = self.client.search(self.query, search_depth=\"advanced\", topic=\"news\", max_results=max_results)\n",
    "        # Return the results\n",
    "        search_response = [{\"href\": obj[\"url\"], \"body\": obj[\"content\"]} for obj in results.get(\"results\", [])]\n",
    "        return search_response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
